{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5964fddf",
   "metadata": {},
   "source": [
    "![{474x427}](GRR.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01d806",
   "metadata": {},
   "source": [
    "### Scraping source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5afbd8",
   "metadata": {},
   "source": [
    "For this data exploration project we used the official race results website for the 2021 edition linked below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e86fbc",
   "metadata": {},
   "source": [
    "https://www.grandraid-reunion.com/francais/resultats/?annee=2021&course=GR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dad9b0",
   "metadata": {},
   "source": [
    "### 1. Scraping the list of athletes who participated in the main race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_results_urls(main_url):\n",
    "    \"\"\"Builds a list of links for all athletes taking part in the race\"\"\"\n",
    "    \n",
    "    # starting time, to keep track of execution time\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Setting options for selenium\n",
    "    options = Options()\n",
    "    options.add_argument('--headless') \n",
    "    options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Scraping the page source containing the list of all athletes and saving it locally\n",
    "    driver.get(main_url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'result-list')))\n",
    "    html = driver.page_source\n",
    "    file_object = open('./results.html', \"w\")\n",
    "    file_object.write(html)\n",
    "    file_object.close()\n",
    "\n",
    "    # Initializing an empty list that will contain all the individual urls    \n",
    "    results_urls = []\n",
    "    \n",
    "    \n",
    "    # parsing the page source for all individual urls\n",
    "    parsed_content = BeautifulSoup(html, 'lxml')\n",
    "    res_list = parsed_content.find(\"ol\", {\"class\": \"result-list\"})\n",
    "    individual_results_list = res_list.findAll(\"li\", {\"class\": \"bold\"})\n",
    "    for result in individual_results_list:\n",
    "        try:\n",
    "            href = result.find(\"a\").get(\"href\")\n",
    "            results_urls.append(href)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Saving the dataset locally to a csv file\n",
    "    df = pd.DataFrame(result_urls)\n",
    "    df.to_csv('./result_urls.csv', index=False)\n",
    "    number_of_links = len(df)\n",
    "    duration = int(time.time() - t0)\n",
    "    print(f\"process completed in {duration} seconds and generated {number_of_links} listings urls\")\n",
    "\n",
    "    return results_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    link = \"https://www.grandraid-reunion.com/francais/resultats/?annee=2021&course=GR\"\n",
    "    result_urls = scrape_results_urls(link)\n",
    "    print(result_urls)\n",
    "    print(len(result_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022db9f5",
   "metadata": {},
   "source": [
    "### 2. Scraping the data for each athlete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_results_urls():\n",
    "    \"\"\"Builds links for all search pages for a given location\"\"\"\n",
    "    \n",
    "    # starting time, to keep track of execution time\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Retrieve the list of individual URLs\n",
    "    urls_df=pd.read_csv('./result_urls.csv')\n",
    "    tot=len(urls_df)\n",
    "\n",
    "    # creating a list to keep track of failed url scraping\n",
    "    errors=[]\n",
    "    \n",
    "    # scraping the html for each individual athlete\n",
    "    for i,row in urls_df.iterrows():\n",
    "        url=row[0]\n",
    "        try:\n",
    "            time.sleep(3)\n",
    "            save_individual_htmls(url, i)\n",
    "            print(f'{i}/{tot} scraping done')\n",
    "        except Exception as e:\n",
    "            print(\"ERROR : \" + str(e))\n",
    "            print(f'{i}/{tot} failed')\n",
    "            errors.append(i)\n",
    "\n",
    "            \n",
    "    # print useful execution information at the end\n",
    "    duration = int(time.time() - t0)\n",
    "    print(f\"process completed in {duration} seconds and {len(errors)} urls failed\")\n",
    "    print(errors)\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "def save_individual_htmls(url,num):\n",
    "   \n",
    "    # Setting options for selenium\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Scraping the URL\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'resume')))\n",
    "\n",
    "    # Saving the page source locally. A tricky situation here as some results had sepcial caracters \n",
    "    # and needed to be utf-8 encoded\n",
    "    try:\n",
    "        file_object = open(f'./results/{num}.html', \"w\")\n",
    "    except:\n",
    "        file_object = open(f'./results/{num}.html', \"w\", encoding=\"utf-8\")\n",
    "    html = driver.page_source\n",
    "    file_object.write(html)\n",
    "    file_object.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result_urls = scrape_results_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b6489",
   "metadata": {},
   "source": [
    "### 3. Parsing individual data to create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd75bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "def extract_element_data(num):\n",
    "    \"\"\"Extracts data from a specified HTML element\"\"\"\n",
    "\n",
    "    # try / except to catch different encodings \n",
    "        try:\n",
    "            file_object = open(f'./results/{i}.html', \"r\")\n",
    "            parsed_content = BeautifulSoup(file_object, 'lxml')\n",
    "\n",
    "        except:\n",
    "            file_object = open(f'./results/{i}.html', \"r\", encoding=\"utf-8\")\n",
    "            parsed_content = BeautifulSoup(file_object, 'lxml')\n",
    "\n",
    "        \n",
    "    # Parsing html with beautifulsoup\n",
    "    parsed_content = BeautifulSoup(file_object, 'lxml')\n",
    "\n",
    "    # Feature dossard, nom, prénom\n",
    "    cat = parsed_content.find(\"div\", {\"class\": \"cat\"}).getText(strip=True)\n",
    "    cat = cat.replace('Infos coureurDossard ', '')\n",
    "    cat = cat.replace('Infos coureurPalmarèsDossard ', '')\n",
    "    cat = cat.split('   Diagonale des fous - ')\n",
    "    num_dossard = int(cat[0])\n",
    "    pnom = parsed_content.find(\"span\", {\"class\": \"pnom\"}).getText()\n",
    "    nom = parsed_content.find(\"span\", {\"class\": \"nom\"}).getText()\n",
    "\n",
    "    # initializing a athlete list \n",
    "    athlete = [num_dossard, pnom, nom]\n",
    "    \n",
    "    \n",
    "    # features 'genre', 'cat', 'club', 'nationalité', 'ville', 'pays','etat', \n",
    "                        'vit_moy'\n",
    "    personal_info = parsed_content.find(\"div\", {\"class\": \"col2\"})\n",
    "    tds = personal_info('td')\n",
    "    for i, td in enumerate(tds):\n",
    "        if i in [1, 3, 5, 7, 9, 11]:\n",
    "            athlete.append(td.getText(strip=True))\n",
    "\n",
    "    # features different for women (global rankin and women ranking) and men    \n",
    "    # features 'class_gen', 'class_F','class_cat', 'arrivée', 'tps_course',\n",
    "    table_resume = parsed_content.find(\"table\", {\"class\": \"resume\"})\n",
    "    resume_rows = table_resume('tr')\n",
    "    for i, row in enumerate(resume_rows):\n",
    "        for j, td in enumerate(row('td')):\n",
    "            athlete.append(td.getText(strip=True))\n",
    "    if athlete[3]=='Femme':\n",
    "        athlete[9] = athlete[9].replace('Etat', '')\n",
    "        athlete[10] = int(athlete[10][-1])\n",
    "        athlete[11] = int(athlete[11][-1])\n",
    "        athlete[12] = int(athlete[12][-1])\n",
    "        athlete[13] = athlete[13].replace('Dernier pointVe. ', '')[:5]\n",
    "        if athlete[9] != \"Arrêté\":\n",
    "            athlete[14] = athlete[14].replace('Temps de course', '')\n",
    "            athlete[15] = athlete[15].replace('Vitesse', '')\n",
    "            athlete[15] = athlete[15].replace(' km/h', '')\n",
    "            athlete[15] = float(athlete[15].replace(',', '.'))\n",
    "    else:\n",
    "        athlete[9] = athlete[9].replace('Etat', '')\n",
    "        athlete[10] = int(athlete[10][-1])\n",
    "        athlete[11] = int(athlete[11][-1])\n",
    "        athlete[12] = athlete[12].replace('Dernier pointVe. ', '')[:5]\n",
    "        if athlete[9]!=\"Arrêté\":\n",
    "            athlete[13] = athlete[13].replace('Temps de course', '')\n",
    "            athlete[14] = athlete[14].replace('Vitesse', '')\n",
    "            athlete[14] = athlete[14].replace(' km/h', '')\n",
    "            athlete[14] = float(athlete[14].replace(',', '.'))\n",
    "            athlete.insert(11,'')\n",
    "\n",
    "    # Perf num_dossard, Point_de_passage, Vitesse, Class., Jour/heure_de_passage, tps_course\n",
    "    table_tpasse = parsed_content.find(\"table\", {\"class\": \"tpass\"})\n",
    "    rows = table_tpasse('tr')\n",
    "    rows_content = []\n",
    "    for i, row in enumerate(rows):\n",
    "        list = []\n",
    "        if i == 0:\n",
    "            for th in row('th'):\n",
    "                if th.getText() == '\\n\\t\\t\\t\\t\\t\\t\\tTemps de course\\t\\t\\t\\t\\t\\t\\t':\n",
    "                    list.append('Temps de course')\n",
    "                else:\n",
    "                    list.append(th.getText())\n",
    "                # print(list )\n",
    "            rows_content.append(list)\n",
    "        else:\n",
    "            list = [num_dossard]\n",
    "            for j, td in enumerate(row('td')):\n",
    "                if j == 0:\n",
    "                    list.append(td.find('a').getText())\n",
    "                else:\n",
    "                    list.append(td.getText(strip=True))\n",
    "            rows_content.append(list)\n",
    "\n",
    "    perf = rows_content[1:]\n",
    "\n",
    "    return athlete, perf\n",
    "\n",
    "\n",
    "def parse_individuals():\n",
    "    directory_path = os.getcwd()\n",
    "    results_directory = join(directory_path, 'results')\n",
    "    file_list = [f for f in listdir(results_directory) if isfile(join(results_directory, f))]\n",
    "    target_directory = join(directory_path, 'parsed_results')\n",
    "    datasets_directory = join(directory_path, 'datasets')\n",
    "\n",
    "    athletes_columns = ['num_dossard', 'pname', 'name', 'genre', 'cat', 'club', 'nationalité', 'ville', 'pays','etat', 'class_gen', 'class_F','class_cat', 'arrivée', 'tps_course',\n",
    "                        'vit_moy' ]\n",
    "\n",
    "    performances_columns = ['num_dossard', 'Point_de_passage', 'Vitesse', 'Class.', 'Jour/heure_de_passage',\n",
    "                            'tps_course']\n",
    "\n",
    "    athletes = []\n",
    "    performances = []\n",
    "    errors=[]\n",
    "    for i in range(len(file_list)):\n",
    "        try:\n",
    "            athlete, performance = extract_element_data(i)\n",
    "            athletes.append(athlete)\n",
    "            for lign in performance:\n",
    "                performances.append(lign)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR : \" + str(e))\n",
    "            print(i)\n",
    "            errors.append(i)\n",
    "\n",
    "\n",
    "    # Save Athletes dataset to CSV\n",
    "    athletes_df = pd.DataFrame(athletes)\n",
    "    athletes_df.columns = athletes_columns\n",
    "    athletes_df.to_csv(join(datasets_directory, 'athletes.csv'), header=True, index=False)\n",
    "    print(athletes_df.head())\n",
    "\n",
    "    # Save Performancet dataset to CSV\n",
    "    performances_df = pd.DataFrame(performances)\n",
    "    performances_df.columns = performances_columns\n",
    "    performances_df.to_csv(join(datasets_directory, 'performances.csv'), header=True, index=False)\n",
    "    print(performances_df.head())\n",
    "\n",
    "    # Print errors\n",
    "    print(errors)\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parse_individuals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b3e18",
   "metadata": {},
   "source": [
    "### 4. stations dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4472e8",
   "metadata": {},
   "source": [
    "There are several resupply stations along the way that we can use to define segements with distances, D+ (ascending elevation), altitude and GPS coordinates. Those pieces of information were garner manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95caabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def create_etapes_dataset():\n",
    "    directory_path = os.getcwd()\n",
    "    datasets_directory = join(directory_path, 'datasets')\n",
    "\n",
    "    columns = ['NOM ETAPE','ETAPE PREC.','NUM ETAPE','ALTITUDE','DIST. DEPUIS LE DÉPART (KM)','DIST. DEPUIS PRECEDENT (KM)','D+ DEPUIS LE DÉPART (M)','D+ DEPUIS DOMAINE PRECEDENT (M)','LATITUDE','LONGITUDE']\n",
    "\n",
    "    etapes=[\n",
    "        ['ST PIERRE RAVINE BLANCHE','',0,3,0,0,0,0,-21.334486634175683, 55.46257224095708],\n",
    "        ['DOMAINE VIDOT', 'ST PIERRE RAVINE BLANCHE',1, 650, 14.6, 14.6,663, 663,-21.312117904695366, 55.5485616153416 ],\n",
    "        ['NOTRE DAME DE LA PAIX', 'DOMAINE VIDOT',2, 1602, 24.9, 10.4, 1687, 1024,-21.248885092968194, 55.5989935020165],\n",
    "        ['PARKING AIRE NEZ DE BOEUF', 'NOTRE DAME DE LA PAIX',3, 2018, 38.8, 13.9, 2440, 753,-21.204995900932236, 55.61679541220081],\n",
    "        ['MARE À BOUE', 'PARKING AIRE NEZ DE BOEUF',4, 1601, 49.2, 10.4, 2511, 71,-21.160180006341562, 55.570757297488704],\n",
    "        ['CILAOS (STADE)', 'MARE À BOUE',5, 1207, 66.1, 16.9, 3336, 825,-21.13501902519813, 55.47481625251336],\n",
    "        ['SENTIER TAÏBIT (DÉBUT)', 'CILAOS (STADE)',6, 1262, 72.7, 6.6, 3833, 497,-21.116965104203345, 55.45064667616225],\n",
    "        ['MARLA', 'SENTIER TAÏBIT (DÉBUT)',7, 1614, 78.7, 6.0, 4665, 832,-21.102651161664944, 55.433699698316744],\n",
    "        ['PLAINE DES MERLES', 'MARLA', 8,1809, 86.3, 7.6, 5161, 496,-21.073755786845823, 55.451307726678124],\n",
    "        ['SENTIER SCOUT', 'PLAINE DES MERLES',9, 1632, 88.4, 2.1, 5174, 13,-21.052765042179438, 55.45441797694862],\n",
    "        ['ILET À BOURSE', 'SENTIER SCOUT', 10,888, 96.4, 8.0, 5389, 215,-21.04202884991624, 55.427462873842174],\n",
    "        ['GD PLACE LES BAS / ÉCOLE', 'ILET À BOURSE',11, 665, 99.7, 3.3, 5534, 145,-21.038496593930127, 55.40971669195356],\n",
    "        ['PLATEAU DE CERF - ROCHE PLATE', 'GD PLACE LES BAS / ÉCOLE',12, 1253, 108, 8.3, 6653, 1119,-21.073617934534838, 55.40078280783869],\n",
    "        ['ILET DES ORANGERS', 'PLATEAU DE CERF - ROCHE PLATE',13, 983, 113.5, 5.4, 6969, 316,-21.042491837243197, 55.39236130384289],\n",
    "        ['DEUX BRAS', 'ILET DES ORANGERS',14, 258, 121.9, 8.4, 7258, 289,-21.003386089319438, 55.39419904599862],\n",
    "        ['DOS D\\'ANE', 'DEUX BRAS',15, 258, 126.6, 4.8, 7963, 705,-20.98057473820627, 55.37216110851838],\n",
    "        ['CHEMIN RATINAUD', 'DOS D\\'ANE',16, 467, 130, 3.4, 7968, 5,-20.963347037690994, 55.35869896813858],\n",
    "        ['LA POSSESSION', 'CHEMIN RATINAUD',17, 3, 138.8, 8.7, 8151, 183,-20.92652865916658, 55.33758113789248],\n",
    "        ['GRANDE CHALOUPE', 'LA POSSESSION',18, 10, 146.1, 7.3, 8518, 367,-20.89554066710821, 55.37737164215647],\n",
    "        ['LE COLORADO', 'GRANDE CHALOUPE',19, 871, 155.5, 9.5, 9356, 838,-20.906137318196727, 55.42508680643403],\n",
    "        ['ST DENIS LA REDOUTE', 'LE COLORADO',20, 53, 160.2, 4.7, 9377, 21,-20.884480141429272, 55.441974429782874],\n",
    "    ]\n",
    "\n",
    "    etapes_df = pd.DataFrame(etapes)\n",
    "    etapes_df.columns = columns\n",
    "    etapes_df.to_csv(join(datasets_directory, 'etapes.csv'), header=True, index=False)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_etapes_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362b60e",
   "metadata": {},
   "source": [
    "All codes and data are available here : https://github.com/afrancoisdata/GRR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
